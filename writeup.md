## Project: Perception Pick & Place
### Writeup Template: You can use this file as a template for your writeup if you want to submit it as a markdown file, but feel free to use some other method and submit a pdf if you prefer.

---


# Required Steps for a Passing Submission:
1. Extract features and train an SVM model on new objects (see `pick_list_*.yaml` in `/pr2_robot/config/` for the list of models you'll be trying to identify). 
2. Write a ROS node and subscribe to `/pr2/world/points` topic. This topic contains noisy point cloud data that you must work with.
3. Use filtering and RANSAC plane fitting to isolate the objects of interest from the rest of the scene.
4. Apply Euclidean clustering to create separate clusters for individual items.
5. Perform object recognition on these objects and assign them labels (markers in RViz).
6. Calculate the centroid (average in x, y and z) of the set of points belonging to that each object.
7. Create ROS messages containing the details of each object (name, pick_pose, etc.) and write these messages out to `.yaml` files, one for each of the 3 scenarios (`test1-3.world` in `/pr2_robot/worlds/`).  [See the example `output.yaml` for details on what the output should look like.](https://github.com/udacity/RoboND-Perception-Project/blob/master/pr2_robot/config/output.yaml)  
8. Submit a link to your GitHub repo for the project or the Python code for your perception pipeline and your output `.yaml` files (3 `.yaml` files, one for each test world).  You must have correctly identified 100% of objects from `pick_list_1.yaml` for `test1.world`, 80% of items from `pick_list_2.yaml` for `test2.world` and 75% of items from `pick_list_3.yaml` in `test3.world`.
9. Congratulations!  Your Done!

## [Rubric](https://review.udacity.com/#!/rubrics/1067/view) Points
### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  

---
### Writeup / README

#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  

You're reading it!

### Exercise 1, 2 and 3 pipeline implemented
#### 1. Complete Exercise 1 steps. Pipeline for filtering and RANSAC plane fitting implemented.

In Exercise 1, the pipeline for filtering and plane fitting is implemented to extract the image segment that belong to objects from a Point Cloud image file.

The input image contains Point Cloud data along with the RGB values for each point:

<img src="./misc_img/ex1-input.png" width="500">

First, the [Voxel Grid Filter](http://pointclouds.org/documentation/tutorials/voxel_grid.php) is applied to the source image to downsample the data size. Using this method enables us to reduce the amount of time for the calculation, while keeping the necessary information to perform object recognition.

The value 0.01 (1cm) is used for the voxel size (leaf size) which seems to be a resonable choice for both the computational time and the amount of information.

The filtered image is shown below:

<img src="./misc_img/ex1-voxel_grid.png" width="500">

Next, the [Pass Through Filter](http://pointclouds.org/documentation/tutorials/passthrough.php) is applied to the downsampled image. This algorithm simply crops the data by specifying an axis with cut-off values along that axis.

Applying the filter with the range 0.6 to 1.1 along z axis gives us the result of retaining only the tabletop and the objects of interest.

<img src="./misc_img/ex1-passthrough.png" width="500">

The final step in the Excercese 1 is to remove the table itself from the scene. To do this we use a popular algorithm known as [RANSAC (RANdom SAmple Consensus)](https://en.wikipedia.org/wiki/Random_sample_consensus). RANSC is an algorithm to identify the points that belong to a certain mathematical model. By modeling the table as a plane, the algorism identifies point cloud data that does not belong to the model as outliers, which can be expected that they belong to objects of interest.

0.01 (1cm) is used for the distance threshold of RANSAC algorithm. The result is shown blow:

- inlier (tabletop)

<img src="./misc_img/ex1-inlier.png" width="500">

- outlier (objects)

<img src="./misc_img/ex1-outlier.png" width="500">

#### 2. Complete Exercise 2 steps: Pipeline including clustering for segmentation implemented.  

In Excersize 2 and following projects, the pipeline is implemented in a  simulated environment generated by [Gazebo](http://gazebosim.org/).

Below is an image of the generated scenes:

<img src="./misc_img/ex2-scene.png" width="500">

After segmenting object point cloud using the same method implemented in Exercise 1, we need to cluster the point cloud that belong to individual objects. To do this, we use the [DBSCAN algorithm](https://en.wikipedia.org/wiki/DBSCAN#Algorithm).

The DBSCAN algorithm, which is also called as Euclidean Clustering algorithm, is an algorithm that clusters data based on its density. It creates clusters by grouping data points that are winthin some threshold distance from the nearest other point in the data. Only the groups that have more than some threshold number of points are recognized as clusters, while others are marked as outliers.

For the threshold values, 0.02 and 30 is used for the distance threshold and the minimum cluster size threshold, respectively. The [k-d tree](https://en.wikipedia.org/wiki/K-d_tree) is used for the nearest neighbor search method.

The pipeline successfully clustered the point cloud into subsets that corresponds to each object:

<img src="./misc_img/ex2-clustered.png" width="500">

#### 3. Complete Exercise 3 Steps.  Features extracted and SVM trained.  Object recognition implemented.

The final task to do with this Exercise is to classify the clustered point clouds into given object models.

There are 6 object models present in this Exercise:

```py
models = [\
   'beer',
   'bowl',
   'create',
   'disk_part',
   'hammer',
   'plastic_cup',
   'soda_can']
```

A supervised machine learning model [Support Vector Machine (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine)  is used to perform the classification. First, a SVM classifier is trained using a training set of features and labels collected from a training environment. Then the trained classifier is used to predict what object models are in the clusteres of point cloud.

Training features are cllected from another Gazebo environment which is an empty environment with only the sensor stick robot and a target object in the scene.

<img src="./misc_img/ex3-training.png" width="400">

In the training environment, target objects are generated in random orientations and their  features are computed based on the point clouds resulting from each of the random orientations. Features are collected with 100 pose variations for each object.

<img src="./misc_img/ex3-training.gif" width="300">

We use color and normal histograms of point cloud for the input features to the classifier. 
Although color values are captured from the sensor stick as RGB values, since HSV representation of the color values is more robust to lighting conditions than RGB representation, they are converted to HSV values while collecting the features.

Color histograms and normal histograms are normalized to the range [0, 256] and [-1, 1] respectively.

In the next step we train the SVM classifier with the collected features using the [code provided by Udacity](https://github.com/udacity/RoboND-Perception-Exercises/blob/master/Exercise-3/sensor_stick/scripts/train_svm.py).

Appliying the trained classifer in the test environment, we get a result shown below. The classifier generally recognizes objects well, but some objects are miss-clustered or miss-classified:

<img src="./misc_img/ex3-result.png" width="500">

### Pick and Place Setup

#### 1. For all three tabletop setups (`test*.world`), perform object recognition, then read in respective pick list (`pick_list_*.yaml`). Next construct the messages that would comprise a valid `PickPlace` request output them to `.yaml` format.

Since there are different object models included in the test world 1~3, the SVM classifier is trained on the new models shown below:

```py
models = [\
   'biscuits',
   'soap',
   'soap2',
   'book',
   'glue',
   'sticky_notes',
   'snacks',
   'eraser']
```

To improve the accuracy of the classifier, we collect the features of target models with 500 different orientations in this setup.

The object recognintion pipeline is implemented the same way as in previous excerceises. However, input data in this setup contain noises which might cause failures at various stages in our perception pipeline. Those noises should be removed in order for the pipeline to perform correctly. The [Statistical Outlier Filter](http://pointclouds.org/documentation/tutorials/statistical_outlier.php) is applied to remove the noises.

Implementions are in [project_template.py](https://github.com/shingo-uzuki/RoboND-Perception-Project/blob/master/pr2_robot/scripts/project_template.py) file. 

#### Result
The perception pipeline recognized all objects (100%) in all the setups (`test[1-3].world`). The below results show the taget objects specified in the pick list, output `.yaml` file, and a screenshot of the recognition result in each setup:

##### `test1.world`

- target objects (3 objects)
  - biscuits
  - soap
  - soap2

- [output1.yaml](https://github.com/shingo-uzuki/RoboND-Perception-Project/blob/master/output/output1.yaml)

- screenshot

<img src="./misc_img/test1_result.png" width="500">

##### `test2.world`

- target objects (5 objects)
  - biscuits
  - soap
  - soap2
  - book
  - glue

- [output2.yaml](https://github.com/shingo-uzuki/RoboND-Perception-Project/blob/master/output/output2.yaml)

- screenshot

<img src="./misc_img/test2_result.png" width="500">

##### `test3.world`

- target objects (5 objects)
  - biscuits
  - soap
  - soap2
  - book
  - glue
  - sticky notes
  - snacks
  - eraser

- [output3.yaml](https://github.com/shingo-uzuki/RoboND-Perception-Project/blob/master/output/output3.yaml)

- screenshot

<img src="./misc_img/test3_result.png" width="500">

#### Improvements

The pipeline often misclassified 'glue' in the `test3.world`. Since 'glue' is occuded and  only the half of its surface is visible to the sensor, it seems that this situation made the classifer to correctly classify such an incomplete point cloud. Randomness of noises retained after the filtering also seems to affecting the result.

The accuracy may be improved by including occlusion and random noise to the features used for  the training set.
